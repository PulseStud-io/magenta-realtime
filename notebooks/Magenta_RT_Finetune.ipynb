{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HcmLuWtE213"
      },
      "source": [
        "# Finetuning Magenta RT\n",
        "\n",
        "\u003ca href=\"https://colab.research.google.com/github/magenta/magenta-realtime/blob/main/notebooks/Magenta_RT_Finetuning.ipynb\" target=\"_parent\"\u003e\u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/\u003e\u003c/a\u003e\n",
        "\n",
        "This notebook will guide you through the steps to finetune Magenta RealTime on your own audio dataset.\n",
        "\n",
        "Magenta RealTime is a Python library for streaming music audio generation on\n",
        "your local device. It is the open weights / on device companion to\n",
        "[MusicFX DJ Mode](https://labs.google/fx/tools/music-fx-dj) and the\n",
        "[Lyria RealTime API](https://ai.google.dev/gemini-api/docs/music-generation).\n",
        "\n",
        "-   [Blog Post](https://g.co/magenta/rt)\n",
        "-   [Repository](https://github.com/magenta/magenta-realtime)\n",
        "-   [HuggingFace](https://huggingface.co/google/magenta-realtime)\n",
        "\n",
        "### Finetuning Magenta RT\n",
        "For finetuning, we recommend at least ~30 minutes of audio of consistent style, but you're free to experiment with something different. Finetuning allows you to build a custom model that sounds uniquely yours, so curating your own training data, and experimenting with it, is an important part of the process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP-rwG3Uzz_1"
      },
      "source": [
        "# Step 1: üò¥ One-time setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qJ4kA_ZEtdsA"
      },
      "outputs": [],
      "source": [
        "# @title **Run this cell** to install dependencies (~5 minutes)\n",
        "# @markdown Make sure you are running on **`v2-8 TPU` runtime** via `Runtime \u003e Change Runtime Type`\n",
        "\n",
        "# @markdown Colab may prompt you to restart session. **Wait until the cell finishes running to restart**!\n",
        "\n",
        "# Clone library\n",
        "!git clone https://github.com/magenta/magenta-realtime.git\n",
        "\n",
        "# Magenta RT requires nightly TF builds, but stable may be installed.\n",
        "# Force nightly to take precedence by uninstalling and reinstalling.\n",
        "# Temporary workaround until MusicCoCa supported by TF stable.\n",
        "_all_tf = 'tensorflow tf-nightly tensorflow-cpu tf-nightly-cpu tensorflow-tpu tf-nightly-tpu tensorflow-hub tf-hub-nightly tensorflow-text tensorflow-text-nightly'\n",
        "_nightly_tf = 'tf-nightly==2.20.0.dev20250619 tensorflow-text-nightly==2.20.0.dev20250316 tf-hub-nightly'\n",
        "\n",
        "\n",
        "# Install library and dependencies\n",
        "# If running on TPU (recommended, runs on free tier Colab TPUs):\n",
        "!pip install -e magenta-realtime/[tpu] \u0026\u0026 pip uninstall -y {_all_tf} \u0026\u0026 pip install {_nightly_tf}\n",
        "# Uncomment if running on GPU (requires A100 via Colab Pro):\n",
        "# !pip install -e magenta-realtime/[gpu] \u0026\u0026 pip uninstall -y {_all_tf} \u0026\u0026 pip install {_nightly_tf}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15YXTfWz2Q35"
      },
      "source": [
        "# Step 2: üìÅ Prepare the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "763S1sRn2Rfv",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to load and process the training data\n",
        "\n",
        "# @markdown **Instructions**. Upload your training data to a Google Drive folder or directly to Colab using the file browser on the left, and specify the name of the directory below in `AUDIO_FOLDER_NAME`.\n",
        "# @markdown For example, if you put your audio in a folder called \"Guitar\" in the root directory on Google Drive, select `AUDIO_SOURCE: \"drive\"` and set `AUDIO_FOLDER_NAME: \"Guitar\"`. You can also reference subdirectories on Google Drive, e.g., `AUDIO_FOLDER_NAME: \"MyAudio/Guitar\"`.\n",
        "\n",
        "AUDIO_SOURCE = \"drive\" # @param [\"colab\",\"drive\"]\n",
        "AUDIO_FOLDER_NAME = \"\"  #@param {type:\"string\", \"placeholder\": \"Name of the top-level folder containing your audio data\"}\n",
        "AUDIO_EXTENSIONS = \"wav,mp3,flac,ogg\"  #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "import seqio\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import t5x\n",
        "import clu.data\n",
        "import tensorflow as tf\n",
        "import tensorflow.data as tf_data\n",
        "import tensorflow.io as tf_io\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.io import wavfile\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.cluster import KMeans\n",
        "from IPython.display import display, Audio\n",
        "\n",
        "from magenta_rt import utils\n",
        "from magenta_rt import musiccoca\n",
        "from magenta_rt.finetune import data\n",
        "from magenta_rt.finetune import tasks\n",
        "from magenta_rt import audio as audio_lib\n",
        "\n",
        "\n",
        "if AUDIO_SOURCE == \"drive\":\n",
        "  audio_dir = f'/content/drive/MyDrive/{AUDIO_FOLDER_NAME}'\n",
        "else:\n",
        "  audio_dir = f'/content/{AUDIO_FOLDER_NAME}'\n",
        "\n",
        "AUDIO_DIR = pathlib.Path(audio_dir)\n",
        "if not AUDIO_DIR.is_dir():\n",
        "  raise FileNotFoundError(f\"Audio directory {audio_dir} does not exist\")\n",
        "\n",
        "# Find audio paths\n",
        "audio_extensions = [e.strip() for e in AUDIO_EXTENSIONS.split(',') if len(e.strip()) \u003e 0]\n",
        "if len(audio_extensions) == 0:\n",
        "  raise ValueError(\"No audio extensions specified\")\n",
        "AUDIO_PATHS = []\n",
        "for e in audio_extensions:\n",
        "  AUDIO_PATHS.extend(list(AUDIO_DIR.glob(f'**/*.{e}')))\n",
        "AUDIO_PATHS = sorted(AUDIO_PATHS)\n",
        "\n",
        "# Help user\n",
        "if len(AUDIO_PATHS) == 0:\n",
        "  raise FileNotFoundError(f\"No audio files found in {audio_dir} with extensions {audio_extensions}\")\n",
        "else:\n",
        "  print(f\"Found {len(AUDIO_PATHS)} audio files in {audio_dir}. A few examples:\")\n",
        "  for p in AUDIO_PATHS[:5]:\n",
        "    print('-' * 80)\n",
        "    print(p)\n",
        "    display(Audio(p))\n",
        "  print('...')\n",
        "\n",
        "\n",
        "TASK_NAME = AUDIO_DIR.stem\n",
        "OUTPUT_DIR = str(pathlib.Path(pathlib.Path.cwd() / 'finetune'))\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "  os.makedirs(OUTPUT_DIR)\n",
        "OUTPUT_PATTERN = f'{OUTPUT_DIR}/{TASK_NAME}_examples.recordio'\n",
        "\n",
        "audio_clips = []\n",
        "total_duration = 0.0\n",
        "for i, file_path in enumerate(AUDIO_PATHS):\n",
        "  with open(f'{file_path}', 'rb') as f:\n",
        "    sr, audio = wavfile.read(f)\n",
        "    audio = audio.astype(np.float32) / np.iinfo(np.int32).max\n",
        "    audio_clips.append((audio, sr))\n",
        "    total_duration += len(audio) / sr\n",
        "\n",
        "print(f\"Loaded {len(audio_clips)} audio clips. Total duration: {total_duration:.0f} seconds (~{round(total_duration/60)} minutes)\")\n",
        "\n",
        "\n",
        "print(\"Tokenizing the training data...\")\n",
        "featurizer = data.Featurizer(\n",
        "    min_clip_seconds=2,\n",
        "    include_style_embeddings=True,\n",
        ")\n",
        "\n",
        "records_count = 0\n",
        "with tf_io.TFRecordWriter(OUTPUT_PATTERN) as file_writer:\n",
        "  for audio, sr in tqdm(audio_clips):\n",
        "    inputs = audio_lib.Waveform(audio, sr)\n",
        "    tokenized_iter = featurizer.process(inputs)\n",
        "    for tokenized_example in tokenized_iter:\n",
        "      records_count += 1\n",
        "      file_writer.write(tokenized_example.SerializeToString())\n",
        "\n",
        "print(f'{records_count} records written')\n",
        "feaurized_audio_length = (records_count) * 30\n",
        "print(f'Total duration of featurized audio: {feaurized_audio_length:.0f} seconds ({(feaurized_audio_length/60):.1f} minutes)')\n",
        "\n",
        "\n",
        "print(f\"Registering new Seqio task...\")\n",
        "if TASK_NAME in seqio.TaskRegistry.names():\n",
        "  seqio.TaskRegistry.remove(TASK_NAME)\n",
        "  seqio.TaskRegistry.remove(TASK_NAME+\"_eval\")\n",
        "\n",
        "tasks.register_task(\n",
        "    name=TASK_NAME,\n",
        "    split_to_filepattern={\n",
        "        'train': OUTPUT_PATTERN,\n",
        "        'validation': OUTPUT_PATTERN,\n",
        "    },\n",
        "    reader_cls=tf_data.TFRecordDataset,\n",
        "    acoustic_key='acoustic_tokens',\n",
        "    style_key='style_tokens',\n",
        "    encoder_codec_rvq_depth=4,\n",
        "    decoder_codec_rvq_depth=16,\n",
        "    max_prompt_secs=10,\n",
        ")\n",
        "\n",
        "print(f\"{TASK_NAME} SeqIO task registered\")\n",
        "\n",
        "\n",
        "print(\"Computing statistics on the finetuning style embeddings...\")\n",
        "def decode_fn(record_bytes):\n",
        "  return tf.io.parse_single_example(\n",
        "      record_bytes,\n",
        "      {\"style_embeddings\": tf.io.FixedLenFeature([], dtype=tf.string)}\n",
        "  )\n",
        "\n",
        "audio_style_embeddings = []\n",
        "for batch in tf.data.TFRecordDataset([OUTPUT_PATTERN]).map(decode_fn):\n",
        "  style_embeds = tf.io.parse_tensor(batch['style_embeddings'], out_type=tf.float32).numpy()\n",
        "  audio_style_embeddings.append(np.mean(style_embeds, axis=0))\n",
        "audio_style_embeddings = np.array(audio_style_embeddings)\n",
        "np.save(f'{OUTPUT_DIR}/{TASK_NAME}_style_embeddings.npy', audio_style_embeddings)\n",
        "mean_audio_embed = np.mean(audio_style_embeddings, axis=0)\n",
        "kmeans = KMeans(n_clusters=5, random_state=0, n_init=10)\n",
        "kmeans.fit(audio_style_embeddings)\n",
        "cluster_centroids = kmeans.cluster_centers_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0vfNy47ov2c"
      },
      "source": [
        "# Step 3: üîß Finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TsR531sXowou",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to start finetuning\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from magenta_rt.finetune import finetuner\n",
        "\n",
        "EXPERIMENT_NAME = \"\" # @param {type: \"string\", \"placeholder\": \"Name of your finetuning experiment (optional)\"}\n",
        "MODEL_SIZE = 'large' # @param ['base', 'large']\n",
        "FINETUNING_STEPS = 6000 # @param\n",
        "SAVE_CKP_PERIOD = 1000 # @param\n",
        "\n",
        "if EXPERIMENT_NAME == \"\":\n",
        "  EXPERIMENT_NAME = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "MODEL_OUTPUT_DIR = f\"{OUTPUT_DIR}/{EXPERIMENT_NAME}\"\n",
        "\n",
        "def get_ds_iterator(\n",
        "    mixture_or_task_name: str,\n",
        "    batch_size: int = 8,\n",
        "    use_cached_tasks: bool = False,\n",
        "    shuffle=True,\n",
        "    ):\n",
        "  train_dataset_cfg = t5x.utils.DatasetConfig(\n",
        "      mixture_or_task_name = mixture_or_task_name,\n",
        "      task_feature_lengths = {'inputs': 1006, 'targets': 800},\n",
        "      split = 'train',\n",
        "      batch_size = batch_size,\n",
        "      shuffle = shuffle,\n",
        "      use_cached = use_cached_tasks,\n",
        "      pack = True,\n",
        "      module = None,\n",
        "      seed = 42,\n",
        "  )\n",
        "\n",
        "  train_ds = t5x.utils.get_dataset(\n",
        "      cfg=train_dataset_cfg,\n",
        "      shard_id=0,\n",
        "      num_shards=1,\n",
        "      feature_converter_cls=seqio.EncDecFeatureConverter,\n",
        "  )\n",
        "  train_iter = clu.data.dataset_iterator.TfDatasetIterator(train_ds, checkpoint=False)\n",
        "  return train_iter\n",
        "\n",
        "def plot_training_curves(training_summary):\n",
        "  num_plots = len(training_summary.keys())\n",
        "  fig, axs = plt.subplots(1, num_plots, figsize=(5*num_plots, 5))\n",
        "  for i, (k, v) in enumerate(training_summary.items()):\n",
        "    axs[i].plot([i.value for i in v])\n",
        "    axs[i].set_xlabel('Step')\n",
        "    axs[i].set_ylabel(k)\n",
        "  plt.show()\n",
        "\n",
        "print(\"Setting up the finetuner...\")\n",
        "MRTFinetuner = finetuner.MagentaRTFinetuner(\n",
        "    tag=MODEL_SIZE,\n",
        "    output_dir=MODEL_OUTPUT_DIR,\n",
        ")\n",
        "\n",
        "print(\"Training...\")\n",
        "train_iter = get_ds_iterator(TASK_NAME)\n",
        "\n",
        "MRTFinetuner.train(\n",
        "    train_iter=train_iter,\n",
        "    num_steps=FINETUNING_STEPS,\n",
        "    save_ckpt_period=SAVE_CKP_PERIOD,\n",
        "  )\n",
        "\n",
        "MRTFinetuner.train_summary\n",
        "\n",
        "plot_training_curves(\n",
        "    {\n",
        "        'Loss': MRTFinetuner.loss,\n",
        "        'Accuracy': MRTFinetuner.accuracy,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94S5nEXipBAY"
      },
      "source": [
        "# Step 4: üéö Play with the finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "w6l-_YV0pAHT"
      },
      "outputs": [],
      "source": [
        "# @title Select the checkpoint\n",
        "\n",
        "from ipywidgets import widgets\n",
        "\n",
        "checkpoints = [int(dir.split(\"_\")[1]) for dir in os.listdir(MODEL_OUTPUT_DIR) if dir.startswith(\"checkpoint_\")]\n",
        "checkpoints.sort()\n",
        "\n",
        "checkpoint_to_load = widgets.Dropdown(\n",
        "    options=checkpoints,\n",
        "    value=checkpoints[-1],\n",
        "    description='Checkpoint:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "display(checkpoint_to_load)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvXU2Il4WkBb"
      },
      "source": [
        "**Run the cell below and click the `start` button to begin streaming!**\n",
        "\n",
        "**Instructions**. Type in text prompts or upload an audio file to control the overall style of the generated music in real time. The sliders change the influence of each prompt on the overall output. The other controls change various aspects of the system behavior (expand below for additional information).\n",
        "\n",
        "‚≠ê **Finetuned model - extra features** ‚≠ê This demo includes additional prompt controls compared to the [main Magenta RT demo](https://github.com/magenta/magenta-realtime/blob/main/notebooks/Magenta_RT_Demo.ipynb) that allow you to steer the model towards styles that are more in-distribution with respect to the finetuning dataset. This can be beneficial when the finetuning data has a narrow coverage of sounds compared to pre-training (as is usually the case). Guiding the model towards in-distribution styles can turn the limited size and diversity of the finetuning data into a feature rather than a bug: out-of-domain prompts mixed with in-distribution embeddings often result in audio that retains some of the original prompt direction, while also being \"infused\" with sonic characteristics typical of the finetuning domain. Alongside the mean style embedding, we expose some cluster centroids as additional in-distribution prompts to achieve similar effects on more compact subspaces of the overall finetuning space.\n",
        "\n",
        "**Disclaimer**. Magenta RT's training data primarily consists of Western\n",
        "instrumental music. As a consequence, Magenta RT has incomplete coverage of both\n",
        "vocal performance and the broader landscape of rich musical traditions\n",
        "worldwide.\n",
        "\n",
        "\u003cdetails\u003e\n",
        "  \u003csummary\u003eClick to expand for additional information on the controls\u003c/summary\u003e\n",
        "\n",
        "*   **extra_buffering_seconds**: Increase this value if you experience audio\n",
        "    drops during generation. This will come at the expense of a greater latency,\n",
        "    but might help with internet connection issues. *You need to relaunch the\n",
        "    cell if you choose to modify this value*.\n",
        "\n",
        "*   **sampling options**\n",
        "\n",
        "    *   **temperature**: This controls how *chaotic* the model behaves. Low\n",
        "        temperature values (e.g., 0.9) will make the model's choices more\n",
        "        predictable and stable. High values (e.g., 1.5) will encourage more\n",
        "        surprising and experimental musical ideas, but can also lead to\n",
        "        instability.\n",
        "\n",
        "    *   **topk**: This parameter filters the model's vocabulary at each step. It\n",
        "        forces the model to choose its next prediction only from the *k* most\n",
        "        likely options.\n",
        "\n",
        "        *   A **low `topk`** value (e.g., 40) restricts the model to a smaller,\n",
        "            safer palette of options. This leads to more coherent and\n",
        "            predictable music that is less likely to have dissonant errors, but\n",
        "            can sometimes feel repetitive.\n",
        "        *   A **high `topk`** value gives the model a much wider range of\n",
        "            choices, allowing for more variety and unexpected turns. This can\n",
        "            make the output more creative, but also noisier.\n",
        "\n",
        "    *   **guidance**: This controls how strictly the generated music should\n",
        "        adhere to the **text prompts**.\n",
        "\n",
        "        *   A **higher value** will push the model to produce a textbook example\n",
        "            of the chosen style, emphasizing its key characteristics.\n",
        "        *   A **lower value** will treat the text prompts more as a loose\n",
        "            inspiration, allowing the model more creative freedom to deviate and\n",
        "            blend other influences.\n",
        "\n",
        "*   **Reset**: stop audio, and resets the model.\n",
        "\n",
        "*   **In-distribution steering**: This allows you to steer the model towards the finetuning style distribution.\n",
        "    The first slider controls the weight of the embedding corresponding to the mean style prompt, the remaining five each correspond to cluster centroids.\n",
        "\n",
        "*   **Text prompts**: Next to each text prompt is a slider that controls how\n",
        "    much each prompt should be affecting the model. This allows the creation of\n",
        "    *mixed* embeddings (try mixing synthwave and flamenco guitar together !).\n",
        "    You can also type your own prompt and modify existing ones.\n",
        "\n",
        "*   **Audio prompts**: Instead of using text to define a musical style, you can\n",
        "    also upload audio references! Click on the `Upload audio file` button to\n",
        "    create a new audio-based prompt. Note that only **the first 10s** of audio\n",
        "    will be used. Supported formats include `.wav`, `.mp3` and `.ogg`.\n",
        "\n",
        "\u003c/details\u003e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bZ92jCA7qzRQ"
      },
      "outputs": [],
      "source": [
        "# @title **Run this cell** to load the selected checkpoint and start the demo\n",
        "\n",
        "import abc\n",
        "import asyncio\n",
        "import concurrent.futures\n",
        "import functools\n",
        "import io\n",
        "import queue\n",
        "import threading\n",
        "import traceback\n",
        "from typing import Sequence\n",
        "\n",
        "import IPython.display as ipd\n",
        "import ipywidgets as ipw\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "\n",
        "from magenta_rt import audio as audio_lib\n",
        "from magenta_rt.colab import utils as colab_utils\n",
        "from magenta_rt.colab import widgets\n",
        "from magenta_rt import system\n",
        "\n",
        "extra_buffering_seconds = 0  # @param {\"type\":\"slider\",\"min\":0,\"max\":4,\"step\":0.1}\n",
        "BUFFERING_AMOUNT_SAMPLES = int(np.ceil(extra_buffering_seconds * 48000))\n",
        "\n",
        "checkpoint_dir = f'{MODEL_OUTPUT_DIR}/checkpoint_{checkpoint_to_load.value}'\n",
        "print(\"Loading checkpoint from \", checkpoint_dir)\n",
        "\n",
        "MRT = system.MagentaRT(\n",
        "    tag=MODEL_SIZE, device=\"tpu:v2-8\", skip_cache=True, lazy=False,\n",
        "    checkpoint_dir=checkpoint_dir,\n",
        ")\n",
        "\n",
        "\n",
        "class AudioFade:\n",
        "  \"\"\"Handles the cross fade between audio chunks.\n",
        "\n",
        "  Args:\n",
        "    chunk_size: Number of audio samples per predicted frame (current\n",
        "      SpectroStream models produces 25Hz frames corresponding to 1920 audio\n",
        "      samples at 48kHz)\n",
        "    num_chunks: Number of audio chunks to fade between.\n",
        "    stereo: Whether the predicted audio is stereo or mono.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, chunk_size: int, num_chunks: int, stereo: bool):\n",
        "    fade_size = chunk_size * num_chunks\n",
        "    self.fade_size = fade_size\n",
        "    self.num_chunks = num_chunks\n",
        "\n",
        "    self.previous_chunk = np.zeros(fade_size)\n",
        "    self.ramp = np.sin(np.linspace(0, np.pi / 2, fade_size)) ** 2\n",
        "\n",
        "    if stereo:\n",
        "      self.previous_chunk = self.previous_chunk[:, np.newaxis]\n",
        "      self.ramp = self.ramp[:, np.newaxis]\n",
        "\n",
        "  def reset(self):\n",
        "    self.previous_chunk = np.zeros_like(self.previous_chunk)\n",
        "\n",
        "  def __call__(self, chunk: np.ndarray) -\u003e np.ndarray:\n",
        "    chunk[: self.fade_size] *= self.ramp\n",
        "    chunk[: self.fade_size] += self.previous_chunk\n",
        "    self.previous_chunk = chunk[-self.fade_size :] * np.flip(self.ramp)\n",
        "    return chunk[: -self.fade_size]\n",
        "\n",
        "\n",
        "class AudioStreamer(abc.ABC):\n",
        "  \"\"\"Audio streamer base class.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      sample_rate: int = 48000,\n",
        "      num_channels: int = 2,\n",
        "      buffer_size: int = 48000 * 2,\n",
        "      extra_buffering: int = BUFFERING_AMOUNT_SAMPLES,\n",
        "  ):\n",
        "    self.audio_streamer = None\n",
        "    self.sample_rate = sample_rate\n",
        "    self.num_channels = num_channels\n",
        "    self.buffer_size = buffer_size\n",
        "    self.extra_buffering = extra_buffering\n",
        "\n",
        "  def on_stream_start(self):\n",
        "    \"\"\"Called when the UI starts streaming.\"\"\"\n",
        "    if self.audio_streamer is not None:\n",
        "      self.audio_streamer.reset_ring_buffer()\n",
        "\n",
        "  def on_stream_stop(self):\n",
        "    \"\"\"Called when the UI stops streaming.\"\"\"\n",
        "    pass\n",
        "\n",
        "  @property\n",
        "  @abc.abstractmethod\n",
        "  def warmup(self) -\u003e bool:\n",
        "    \"\"\"Returns whether to warm up the audio streamer.\"\"\"\n",
        "    pass\n",
        "\n",
        "  def reset(self):\n",
        "    if self.audio_streamer is not None:\n",
        "      self.audio_streamer.reset_ring_buffer()\n",
        "\n",
        "  def start(self):\n",
        "    self.audio_streamer = colab_utils.AudioStreamer(\n",
        "        self,\n",
        "        rate=self.sample_rate,\n",
        "        buffer_size=self.buffer_size,\n",
        "        warmup=self.warmup,\n",
        "        num_output_channels=self.num_channels,\n",
        "        additional_buffered_samples=self.extra_buffering,\n",
        "        start_streaming_callback=self.on_stream_start,\n",
        "        stop_streaming_callback=self.on_stream_stop,\n",
        "    )\n",
        "    self.reset()\n",
        "\n",
        "  def stop(self):\n",
        "    if self.audio_streamer is not None:\n",
        "      del self.audio_streamer\n",
        "      self.audio_streamer = None\n",
        "\n",
        "  def global_ui_params(self):\n",
        "    return colab_utils.Parameters.get_values()\n",
        "\n",
        "  def get_prompts(self):\n",
        "    params = self.global_ui_params()\n",
        "    num_prompts = sum(map(lambda s: \"prompt\" in s, params.keys()))\n",
        "    prompts = []\n",
        "    for i in range(num_prompts):\n",
        "      weight = params[f\"prompt_{i}\"]\n",
        "      if not weight:\n",
        "        continue\n",
        "      text_or_audio = params[f\"style_{i}\"]\n",
        "\n",
        "      if text_or_audio is None:\n",
        "        continue\n",
        "\n",
        "      if isinstance(text_or_audio, str):\n",
        "        text_or_audio = text_or_audio.strip()\n",
        "      prompts.append((text_or_audio, weight))\n",
        "    return prompts\n",
        "\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def generate(self, ui_params):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    del inputs\n",
        "    return self.generate(self.global_ui_params())\n",
        "\n",
        "\n",
        "class MagentaRTStreamer(AudioStreamer):\n",
        "  \"\"\"Audio streamer class for our open weights Magenta RT model.\n",
        "\n",
        "  This class holds a pretrained Magenta RT model, a cross fade state, a\n",
        "  generation state and an asynchronous executor to handle the embedding of text\n",
        "  prompt without interrupting the audio thread.\n",
        "\n",
        "  Args:\n",
        "    system: A MagentaRTBase instance.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, system: system.MagentaRTBase):\n",
        "    super().__init__()\n",
        "    self.system = system\n",
        "    self.fade = AudioFade(chunk_size=1920, num_chunks=1, stereo=True)\n",
        "    self.state = None\n",
        "    self.executor = concurrent.futures.ThreadPoolExecutor()\n",
        "\n",
        "  @property\n",
        "  def warmup(self):\n",
        "    return True\n",
        "\n",
        "  @functools.cache\n",
        "  def embed_style(self, style: str):\n",
        "    return self.executor.submit(self.system.embed_style, style)\n",
        "\n",
        "  @functools.cache\n",
        "  def embed_audio(self, audio: tuple[float]):\n",
        "    audio = audio_lib.Waveform(np.asarray(audio), 16000)\n",
        "    return self.executor.submit(self.system.embed_style, audio)\n",
        "\n",
        "  def get_style_embedding(\n",
        "      self,\n",
        "      force_wait: bool = False,\n",
        "      mean_audio_embed: np.ndarray | None = None,\n",
        "      cluster_centroids: np.ndarray | None = None,\n",
        "  ):\n",
        "    prompts = self.get_prompts()\n",
        "    weighted_embedding = np.zeros((768,), dtype=np.float32)\n",
        "    total_weight = 0.0\n",
        "    for text_or_audio, weight in prompts:\n",
        "      if not weight:\n",
        "        continue\n",
        "\n",
        "      if isinstance(text_or_audio, np.ndarray):\n",
        "        embedding = self.embed_audio(tuple(text_or_audio))\n",
        "      else:\n",
        "        if not text_or_audio:\n",
        "          continue\n",
        "        embedding = self.embed_style(text_or_audio)\n",
        "\n",
        "      if force_wait:\n",
        "        embedding.result()\n",
        "      if embedding.done():\n",
        "        weighted_embedding += embedding.result() * weight\n",
        "        total_weight += weight\n",
        "\n",
        "    if mean_audio_embed is not None:\n",
        "      params = self.global_ui_params()\n",
        "      training_weight = params[\"mean\"]\n",
        "      weighted_embedding += mean_audio_embed * training_weight\n",
        "      total_weight += training_weight\n",
        "\n",
        "    if cluster_centroids is not None:\n",
        "      params = self.global_ui_params()\n",
        "      for i, centroid in enumerate(cluster_centroids):\n",
        "        centroid_weight = params[f\"centroid_{i}\"]\n",
        "        weighted_embedding += centroid * centroid_weight\n",
        "        total_weight += centroid_weight\n",
        "\n",
        "    if total_weight \u003e 0:\n",
        "      weighted_embedding /= total_weight\n",
        "\n",
        "    return weighted_embedding\n",
        "\n",
        "  def on_stream_start(self):\n",
        "    self.get_style_embedding(force_wait=False)\n",
        "    self.get_style_embedding(force_wait=True)\n",
        "    super().on_stream_start()\n",
        "\n",
        "  def reset(self):\n",
        "    self.state = None\n",
        "    self.fade.reset()\n",
        "    self.embed_style.cache_clear()\n",
        "    super().reset()\n",
        "\n",
        "  def generate(self, ui_params):\n",
        "    chunk, self.state = self.system.generate_chunk(\n",
        "        state=self.state,\n",
        "        style=self.get_style_embedding(\n",
        "            mean_audio_embed=mean_audio_embed,\n",
        "            cluster_centroids=cluster_centroids,\n",
        "        ),\n",
        "        seed=None,\n",
        "        **ui_params,\n",
        "    )\n",
        "    chunk = self.fade(chunk.samples)\n",
        "    return chunk\n",
        "\n",
        "  def stop(self):\n",
        "    self.executor.shutdown(wait=True)\n",
        "\n",
        "# BUILD UI\n",
        "\n",
        "\n",
        "def build_prompt_ui(default_prompts: Sequence[str], num_audio_prompt: int):\n",
        "  \"\"\"Add interactive prompt widgets and register them.\"\"\"\n",
        "  prompts = []\n",
        "\n",
        "  for p in default_prompts:\n",
        "    prompts.append(widgets.Prompt())\n",
        "    prompts[-1].text.value = p\n",
        "\n",
        "  prompts[0].slider.value = 1.0\n",
        "\n",
        "  # add audio prompt\n",
        "  for _ in range(num_audio_prompt):\n",
        "    prompts.append(widgets.AudioPrompt())\n",
        "    prompts[-1].slider.value = 0.0\n",
        "\n",
        "  colab_utils.Parameters.register_ui_elements(\n",
        "      display=False,\n",
        "      **{f\"prompt_{i}\": p.slider for i, p in enumerate(prompts)},\n",
        "      **{f\"style_{i}\": p.prompt_value for i, p in enumerate(prompts)},\n",
        "  )\n",
        "  return [p.get_widget() for p in prompts]\n",
        "\n",
        "def build_steering_option_ui(num_centroids):\n",
        "  \"\"\"Add interactive steering option widgets and register them.\"\"\"\n",
        "  options = {\n",
        "   \"mean\": ipw.FloatSlider(\n",
        "          min=0.0,\n",
        "          max=10.0,\n",
        "          step=0.01,\n",
        "          value=1.0,\n",
        "          description=\"mean\",\n",
        "          layout=ipw.Layout(width='500px'),\n",
        "      ),\n",
        "   }\n",
        "\n",
        "  for i in range(num_centroids):\n",
        "      options[f\"centroid_{i}\"] = ipw.FloatSlider(\n",
        "          min=0.0,\n",
        "          max=10.0,\n",
        "          step=0.01,\n",
        "          value=0.0,\n",
        "          description=f\"centroid {i+1}\",\n",
        "          layout=ipw.Layout(width='500px'),\n",
        "      )\n",
        "\n",
        "  colab_utils.Parameters.register_ui_elements(display=False, **options)\n",
        "\n",
        "  return list(options.values())\n",
        "\n",
        "def build_sampling_option_ui():\n",
        "  \"\"\"Add interactive sampling option widgets and register them.\"\"\"\n",
        "  options = {\n",
        "      \"temperature\": ipw.FloatSlider(\n",
        "          min=0.0,\n",
        "          max=4.0,\n",
        "          step=0.01,\n",
        "          value=1.3,\n",
        "          description=\"temperature\",\n",
        "      ),\n",
        "      \"topk\": ipw.IntSlider(\n",
        "          min=0,\n",
        "          max=1024,\n",
        "          step=1,\n",
        "          value=40,\n",
        "          description=\"topk\",\n",
        "      ),\n",
        "      \"guidance_weight\": ipw.FloatSlider(\n",
        "          min=0.0,\n",
        "          max=10.0,\n",
        "          step=0.01,\n",
        "          value=5.0,\n",
        "          description=\"guidance\",\n",
        "      ),\n",
        "  }\n",
        "\n",
        "  colab_utils.Parameters.register_ui_elements(display=False, **options)\n",
        "\n",
        "  return list(options.values())\n",
        "\n",
        "\n",
        "colab_utils.Parameters.reset()\n",
        "\n",
        "# Initialize streamer\n",
        "try:\n",
        "  MRT\n",
        "except NameError:\n",
        "  raise RuntimeError(\"Please run the cell above.\")\n",
        "streamer = MagentaRTStreamer(MRT)\n",
        "\n",
        "\n",
        "def _reset_state(*args, **kwargs):\n",
        "  del args, kwargs\n",
        "  streamer.reset()\n",
        "\n",
        "\n",
        "reset_button = ipw.Button(description=\"reset\")\n",
        "reset_button.on_click(_reset_state)\n",
        "\n",
        "\n",
        "# Building interactive UI\n",
        "ipd.display(\n",
        "    ipw.VBox([\n",
        "        widgets.area(\n",
        "            \"sampling options\",\n",
        "            *build_sampling_option_ui(),\n",
        "            reset_button,\n",
        "        ),\n",
        "         widgets.area(\n",
        "          \"in-distribution steering\",\n",
        "          *build_steering_option_ui(len(cluster_centroids)),\n",
        "        ),\n",
        "        widgets.area(\n",
        "            \"prompts\",\n",
        "            *build_prompt_ui(\n",
        "                [\n",
        "                    \"synthwave\",\n",
        "                    \"flamenco guitar\",\n",
        "                    \"\",\n",
        "                    \"\",\n",
        "                ],\n",
        "                num_audio_prompt=2,\n",
        "            ),\n",
        "        ),\n",
        "    ])\n",
        ")\n",
        "\n",
        "streamer.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmvBk8UOphdL"
      },
      "source": [
        "# License and disclaimer\n",
        "\n",
        "Magenta RealTime is offered under a combination of licenses: the codebase is\n",
        "licensed under\n",
        "[Apache 2.0](https://github.com/magenta/magenta-realtime/blob/main/LICENSE),\n",
        "and the model weights under\n",
        "[Creative Commons Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/legalcode).\n",
        "\n",
        "In addition, we specify the following usage terms:\n",
        "\n",
        "Copyright 2025 Google LLC\n",
        "\n",
        "Use these materials responsibly and do not generate content, including outputs,\n",
        "that infringe or violate the rights of others, including rights in copyrighted\n",
        "content.\n",
        "\n",
        "Google claims no rights in outputs you generate using Magenta RealTime. You and\n",
        "your users are solely responsible for outputs and their subsequent uses.\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, all software and\n",
        "materials distributed here under the Apache 2.0 or CC-BY licenses are\n",
        "distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,\n",
        "either express or implied. See the licenses for the specific language governing\n",
        "permissions and limitations under those licenses. You are solely responsible for\n",
        "determining the appropriateness of using, reproducing, modifying, performing,\n",
        "displaying or distributing the software and materials, and any outputs, and\n",
        "assume any and all risks associated with your use or distribution of any of the\n",
        "software and materials, and any outputs, and your exercise of rights and\n",
        "permissions under the licenses."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
